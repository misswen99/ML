{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据\n",
    "dataset = pd.read_csv('datasets/Data.csv')\n",
    "\n",
    "X = dataset.iloc[ : , :-1].values\n",
    "Y = dataset.iloc[ : , 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.处理缺失的数据  数据标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填补缺失值：sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)\n",
    "\n",
    "主要参数说明：\n",
    "\n",
    "missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN\n",
    "\n",
    "strategy：替换策略，字符串，默认用均值‘mean’替换\n",
    "\n",
    "①若为mean时，用特征列的均值替换\n",
    "\n",
    "②若为median时，用特征列的中位数替换\n",
    "\n",
    "③若为most_frequent时，用特征列的众数替换\n",
    "\n",
    "axis：指定轴数，默认axis=0代表列，axis=1代表行\n",
    "\n",
    "copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理丢失的数据\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = imputer.fit(X[ : , 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据标准化\n",
    "X[ : , 1:3] = imputer.transform(X[ : , 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.分类数据\n",
    "https://blog.csdn.net/qq_24509229/article/details/80183173\n",
    "\n",
    "将分类数据，转换成数字格式  \n",
    "类别编码(LabelEncoder)  \n",
    "就是把数据变成连续的数值型变量，比如原先有“American”“Japanese”“Chinese”这些的，换成(0,1,2)\n",
    "  \n",
    "独热编码(OnehotEncoder)  \n",
    "就是把数据变成(1,0,0,...,0),(0,1,0,0,...,0)，该特征属性有多少类别就有多少维\n",
    "  \n",
    "**Onehotencoder一般会是特征空间变得很大，但又因为使用Onehotencoder会使特征空间变得稀疏，所以一般都是Onehotencoder+PCA一起用。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编码分类数据\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建虚拟变量\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y =  labelencoder_Y.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.将数据集分为训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data：所要划分的样本特征集\n",
    "\n",
    "train_target：所要划分的样本结果\n",
    "\n",
    "test_size：样本占比，如果是整数的话就是样本的数量\n",
    "\n",
    "random_state：是随机数的种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\wang\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#将数据集分为训练集和测试集\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.特征缩放  \n",
    "数据标准化  \n",
    "ss = sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True) \n",
    "在这里 \n",
    "copy; with_mean;with_std \n",
    "默认的值都是True.\n",
    "\n",
    "copy 如果为false,就会用归一化的值替代原来的值;如果被标准化的数据不是np.array或scipy.sparse CSR matrix, 原来的数据还是被copy而不是被替代\n",
    "\n",
    "with_mean 在处理sparse CSR或者 CSC matrices 一定要设置False不然会超内存\n",
    "\n",
    "能够查询的属性:\n",
    "\n",
    "scale_： 缩放比例，同时也是标准差\n",
    "\n",
    "mean_： 每个特征的平均值\n",
    "\n",
    "var_:每个特征的方差\n",
    "\n",
    "n_sample_seen_:样本数量，可以通过patial_fit 增加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#特征缩放\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "print(sc_X.scale_)\n",
    "print(sc_X.n_samples_seen_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"---------------------\")\n",
    "# print(\"Step 6: Feature Scaling\")\n",
    "# print(\"X_train\")\n",
    "# print(X_train)\n",
    "# print(\"X_test\")\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
